{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Challenge\n",
    "The dataset is a collection of audio clips with expressions of the following emotions: *angry, fear, happy, neutral, sad*.  \n",
    "  \n",
    "Your task is to train a model to perform *speech emotion recognition*.  \n",
    "  \n",
    "You will be provided a training set for your use. You are allowed to include additional data to train your model.  \n",
    "  \n",
    "The evaluation data will consist of audio clips spoken in multiple languages.  \n",
    "*Majority* of the evaluation data are in the intonation of *Singapore English*.  \n",
    "  \n",
    "Take inspo from [training notebook](https://github.com/AbishekSankar/Audio-Classification-Deep-Learning/blob/main/Demo%20Jupyter%20Notebook/Final_Project.ipynb)  \n",
    "and [Speech Emotion Recognition with CNN](https://www.kaggle.com/code/ritzing/speech-emotion-recognition-with-cnn/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Extra Datasets\n",
    "https://www.kaggle.com/datasets/dmitrybabko/speech-emotion-recognition-en (specifically Crema)  \n",
    "https://www.kaggle.com/datasets/piyushagni5/berlin-database-of-emotional-speech-emodb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as lb\n",
    "from librosa.display import specshow\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ms = 4000\n",
    "\n",
    "ind_to_label = {\n",
    "    0 : 'angry',\n",
    "    1 : 'fear',\n",
    "    2 : 'happy',\n",
    "    3 : 'neutral',\n",
    "    4 : 'sad'\n",
    "}\n",
    "\n",
    "label_to_ind = { \n",
    "    lab: ind for ind, lab in ind_to_label.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aud_util:\n",
    "    \n",
    "    @staticmethod\n",
    "    def loadaud(audio_file_path, sr=None, mono=False):                                 # load audio file, *mono argument (bool) can auto convert to mono, while default sr is converted to 22050*\n",
    "        return lb.load(audio_file_path, sr=sr, mono=mono)                              # returns (data, sr)       \n",
    "    \n",
    "    # @staticmethod\n",
    "    # def mono_channel_withsr(audio_data_with_sr):\n",
    "    #     return lb.to_mono(audio_data_with_sr[0]), audio_data_with_sr[1]\n",
    "\n",
    "    # @staticmethod\n",
    "    # def resample_withsr(data, in_sr, new_sr=22050):\n",
    "    #     if in_sr == new_sr:\n",
    "    #         return data\n",
    "    #     else:\n",
    "    #         return lb.resample(data, orig_sr=sr, new_sr=new_sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, sr, target_ms):                                                 # padding places shorter audio randomly within the time frame of the padded length\n",
    "        maxlen = (target_ms//1000)*sr\n",
    "        \n",
    "        if len(aud) == maxlen:\n",
    "            return aud, sr\n",
    "\n",
    "        elif len(aud) > maxlen:\n",
    "            return aud[:maxlen], sr\n",
    "\n",
    "        elif len(aud) < maxlen:\n",
    "            \n",
    "            #     random padding positions\n",
    "            pad = maxlen - len(aud)\n",
    "            pad = np.zeros((pad))\n",
    "\n",
    "            # pad_begin_len = rng.randint(0, maxlen - len(aud))\n",
    "            # pad_end_len = maxlen - len(aud) - pad_begin_len\n",
    "\n",
    "            #     actaual padding\n",
    "            # pad_begin = np.zeros((pad_begin_len))\n",
    "            # pad_end = np.zeros((pad_end_len))\n",
    "\n",
    "            return np.concatenate((aud, pad), 0), sr\n",
    "\n",
    "\n",
    "\n",
    "class aud_img:\n",
    "    @staticmethod\n",
    "    def melspec(data, sr):\n",
    "        spec = lb.feature.melspectrogram(data, sr=sr, power=1)                         # power = 1/2 changes amplitude_to_db or power_to_db\n",
    "        spec = lb.amplitude_to_db(spec, ref=np.min)\n",
    "        return spec\n",
    "\n",
    "    @staticmethod\n",
    "    def mfcc(data, sr):\n",
    "        mfcc_ = lb.feature.mfcc(data, sr)\n",
    "        #mfcc_ = sk.preprocessing.scale(mfcc_, axis=1)\n",
    "        return mfcc_\n",
    "\n",
    "    # @staticmethod\n",
    "    # def display_audio_img(spec, sr , mfcc=False):\n",
    "    #     fig, ax = plt.subplots()\n",
    "        \n",
    "    #     if mfcc:\n",
    "    #         specshow(spec, sr=sr, x_axis='time')\n",
    "    #     else:\n",
    "    #         img = specshow(spec, x_axis='time', y_axis='mel', sr=sr, fmax=8000, ax=ax)\n",
    "    #         fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "\n",
    "\n",
    "\n",
    "class ds_create:\n",
    "    \n",
    "    # @staticmethod    \n",
    "    # def label_from_bpath(bpath):                                                       # probably will not be used\n",
    "    #     return bpath.decode('utf-8').split('\\\\')[-2]\n",
    "\n",
    "    # @staticmethod\n",
    "    # #depreciated\n",
    "    # def one_label_dataset(path, label):                                                # path taken in must be raw\n",
    "    #     return tf.data.Dataset.zip((\n",
    "    #         tf.data.Dataset.list_files(path),\n",
    "    #         tf.data.Dataset.from_tensor_slices(tf.constant(value=label_to_ind[label], dtype=tf.dtypes.int32 ,shape=len(tf.data.Dataset.list_files(path))))\n",
    "    #     ))\n",
    "\n",
    "    @staticmethod\n",
    "    def slices_for_onelabel(path='Data/NLP Training Dataset/', label):                                              #for zhihao's local pc\n",
    "        paths = glob.glob(path + label + '/*.wav')\n",
    "        labels = [label_to_ind[label]]*len(paths)\n",
    "        return paths, labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_mel_eachlabel(file_path, label):                          \n",
    "        \n",
    "        data, sr = aud_util.loadaud(file_path, sr=16000, mono=True)\n",
    "        data, sr = aud_util.pad_trunc(data, sr, max_ms)                                \n",
    "        mel = aud_img.melspec(data, sr)\n",
    "        mel = tf.expand_dims(mel, axis=2)\n",
    "        \n",
    "        return mel, label\n",
    "\n",
    "    # @staticmethod\n",
    "    # def path_to_mel(path):                                                              # temporary work around\n",
    "        \n",
    "    #     data, sr = aud_util.loadaud(path, sr=16000, mono=True)\n",
    "    #     data, sr = aud_util.pad_trunc(data, sr, max_ms)                                \n",
    "    #     mel = aud_img.melspec(data, sr)\n",
    "    #     mel = tf.expand_dims(mel, axis=2)\n",
    "\n",
    "    #     return mel\n",
    "\n",
    "    @staticmethod\n",
    "    def dfpremel(path):\n",
    "        data, sr = aud_util.loadaud(path, sr=16000, mono=True)\n",
    "        data, sr = aud_util.pad_trunc(data, sr, max_ms)                                \n",
    "        mel = aud_img.melspec(data, sr)\n",
    "        mel = np.expand_dims(mel, axis=2)\n",
    "\n",
    "        return mel\n",
    "    \n",
    "    @staticmethod\n",
    "    def dfpremfcc(path):\n",
    "        data, sr = aud_util.loadaud(path, sr=16000, mono=True)\n",
    "        data, sr = aud_util.pad_trunc(data, sr, max_ms)                                \n",
    "        mfcc = aud_img.mfcc(data, sr)\n",
    "        mfcc = np.expand_dims(mfcc, axis=2)\n",
    "\n",
    "        return mfcc\n",
    "\n",
    "    @staticmethod\n",
    "    def dup_channel(img):\n",
    "        return np.stack((img,)*3, axis=2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry, _0= ds_create.slices_for_onelabel('Data/NLP Training Dataset/', 'angry')\n",
    "fear, _1 = ds_create.slices_for_onelabel('Data/NLP Training Dataset/', 'fear')\n",
    "happy, _2 = ds_create.slices_for_onelabel('Data/NLP Training Dataset/', 'happy')\n",
    "neutral, _3 = ds_create.slices_for_onelabel('Data/NLP Training Dataset/', 'neutral')\n",
    "sad, _4 = ds_create.slices_for_onelabel('Data/NLP Training Dataset/', 'sad')\n",
    "\n",
    "slices = angry + fear + happy + neutral + sad\n",
    "labels = _0 + _1 + _2 + _3 + _4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df['relative_audio_paths'] = slices\n",
    "df['int_labels'] = labels\n",
    "df['1hot_labels'] = list(to_categorical(labels))\n",
    "\n",
    "df['imgs_1c'] = list(map(ds_create.dfpremel, slices))\n",
    "df['imgs_3c'] = df['imgs_1c'].map(ds_create.dup_channel)\n",
    "\n",
    "\n",
    "df = sk.utils.shuffle(df)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = df.iloc[0,4].shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xIn = Input(input_shape)\n",
    "net = tf.keras.applications.efficientnet_v2.EfficientNetV2S(weights='imagenet', include_top=False)\n",
    "x = net(xIn)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='swish')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "xOut = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(xIn, xOut)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['acc', tfa.metrics.F1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.1, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint('Model_weights', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "]\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit(\n",
    "    x=tf.stack(df['imgs_3c']),\n",
    "    y=tf.stack(df['1hot_labels']),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Evaluation or Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds_create.dfpremel('NLP/NLP Training Dataset/fear/00530e07e3.wav')\n",
    "test = ds_create.dup_channel(test)\n",
    "test = np.expand_dims(test, axis=0) # EXPAND DIMS OF FIRST DIMENSION ARGHHHHHH\n",
    "pred = model.predict(test)\n",
    "pred = np.argmax(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_gen:\n",
    "\n",
    "    @staticmethod\n",
    "    def path_to_mel(path):\n",
    "        c = ds_create.dfpremel(path)\n",
    "        ccc = ds_create.dup_channel(c)\n",
    "        return ccc\n",
    "    \n",
    "    @staticmethod\n",
    "    def path_to_mfcc(path):\n",
    "        c = ds_create.dfpremfcc(path)\n",
    "        ccc = ds_create.dup_channel(c)\n",
    "        return ccc\n",
    "\n",
    "    @staticmethod\n",
    "    def int_to_label(int):\n",
    "        return ind_to_label[int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df = pd.DataFrame()\n",
    "paths = sorted(glob.glob('Data/NLP Interim Dataset/*.wav'))\n",
    "q_data = list(map(test_gen.path_to_mel, paths))\n",
    "\n",
    "q_data = tf.stack(q_data)\n",
    "\n",
    "preds = model.predict(q_data)\n",
    "preds = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = []\n",
    "# filepath = 'Data/NLP Interim Dataset/NLP Interim Dataset/NLP/*'\n",
    "# for file in glob.glob(filepath):\n",
    "#     feature = extract_feature(file, mfcc=False, chroma=False, mel=True)\n",
    "#     x_test.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract features (mfcc, chroma, mel) from a sound file\n",
    "# def extract_feature(file_name, mfcc, chroma, mel):\n",
    "#     with soundfile.SoundFile(file_name) as sound_file:\n",
    "#         X = sound_file.read(dtype=\"float32\")\n",
    "#         sample_rate=sound_file.samplerate\n",
    "#         if chroma:\n",
    "#             stft=np.abs(librosa.stft(X))\n",
    "#         result=np.array([])\n",
    "#         if mfcc:\n",
    "#             mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "#             result=np.hstack((result, mfccs))\n",
    "#         if chroma:\n",
    "#             chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "#             result=np.hstack((result, chroma))\n",
    "#         if mel:\n",
    "#             mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "#             result=np.hstack((result, mel))\n",
    "#     # print(type(result))\n",
    "#     # print(result.shape)\n",
    "#     return result\n",
    "# x_train, y_train = [], []\n",
    "\n",
    "# for emotion in emotions_list:\n",
    "#     filepath = 'Data/NLP Training Dataset/ASR Training Dataset/' + emotion + '/*'\n",
    "#     for file in glob.glob(filepath):\n",
    "#         feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "#         x_train.append(feature)\n",
    "#         y_train.append(emotion)\n",
    "\n",
    "# x_train = np.array(x_train)\n",
    "# y_train = np.array(y_train)\n",
    "# print(type(x_train))\n",
    "# print(x_train.shape)\n",
    "# # shuffle order of data and label to match\n",
    "# def unison_shuffled_arrays(x, y):\n",
    "#     assert len(x) == len(y)\n",
    "#     p = np.random.permutation(len(x))\n",
    "#     return x[p], y[p]\n",
    "\n",
    "# shuffle_x_train, shuffle_y_train = unison_shuffled_arrays(x_train, y_train)\n",
    "# print(shuffle_x_train.shape)\n",
    "\n",
    "# print(len(shuffle_x_train[5]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6425117d13823fa8044a2c07c859b613f2362d94b282c9a4162ba20339fd2c4d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
