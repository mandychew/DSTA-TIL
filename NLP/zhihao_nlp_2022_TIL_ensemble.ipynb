{
<<<<<<< HEAD:NLP/zhihao_nlp_2022_TIL_ensemble.ipynb
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfTT2tGszAvC"
      },
      "source": [
        "# AGENDA\n",
        "- [ ] Try out other ppretrained models other than effinet\n",
        "- [ ] Experiment with using less dropout on larger models\n",
        "- [ ] data augmentation... and loading training images into google drive (after augmenttation). Augment audio(stretch, loudness, noise) and images(vertical, horizontal bars\n",
        "- [ ] Possibly look into MFCCs again  \n",
        "- [x] model ensembling -- done by Mandy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNub-g9OTBer",
        "outputId": "27327787-1e04-4025-ed45-2b4dd1e14261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.0\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I8g4N2Je3GxB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil as sh\n",
        "import random as rng\n",
        "import glob\n",
        "# import itertools\n",
        "\n",
        "import librosa as lb\n",
        "from librosa.display import specshow\n",
        "import IPython.display as ipd\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "import sklearn as sk\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "n9emmQj03Rsx",
        "outputId": "4cdd23ab-85d6-4f98-86b1-a0e41c9c6e9f"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdIDRxQnaL_K"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def map_to_array(example):\n",
        "    speech, _ = librosa.load(example[\"file\"], sr=16000, mono=True)\n",
        "    example[\"speech\"] = speech\n",
        "    return example\n",
        "\n",
        "# load a demo dataset and read audio files\n",
        "dataset = load_dataset(\"anton-l/superb_demo\", \"er\", split=\"session1\")\n",
        "dataset = dataset.map(map_to_array)\n",
        "\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
        "\n",
        "# compute attention masks and normalize the waveform if needed\n",
        "inputs = feature_extractor(dataset[:4][\"speech\"], sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "logits = model(**inputs).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "labels = [model.config.id2label[_id] for _id in predicted_ids.tolist()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iICR-Ysk3GxE"
      },
      "outputs": [],
      "source": [
        "#  --  Defining Variables  --  #\n",
        "\n",
        "max_ms = 4000\n",
        "\n",
        "batchs = 64\n",
        "epochs = 20\n",
        "\n",
        "ind_to_label = {\n",
        "    0 : 'angry',\n",
        "    1 : 'fear',\n",
        "    2 : 'happy',\n",
        "    3 : 'neutral',\n",
        "    4 : 'sad'\n",
        "}\n",
        "\n",
        "label_to_ind = { \n",
        "    lab: ind for ind, lab in ind_to_label.items()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRm58mWP3GxE"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Data insights\n",
        "\n",
        "'''\n",
        "\n",
        "class aud_stats:\n",
        "    @staticmethod\n",
        "    def average_sr():\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITKWjwvvnZnk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "PREPROCESSING UTILS\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "class aud_util:\n",
        "    \n",
        "    @staticmethod\n",
        "    def loadaud(audio_file_path, sr=None, mono=False):                                 # load audio file, *mono argument (bool) can auto convert to mono, while default sr is converted to 22050*\n",
        "        return lb.load(audio_file_path, sr=sr, mono=mono)                              # returns (data, sr)       \n",
        "\n",
        "    @staticmethod\n",
        "    def pad_trunc(aud, sr, target_ms):                                                 # padding places shorter audio randomly within the time frame of the padded length\n",
        "        maxlen = (target_ms//1000)*sr\n",
        "        \n",
        "        if len(aud) == maxlen:\n",
        "            return aud, sr\n",
        "\n",
        "        elif len(aud) > maxlen:\n",
        "            return aud[:maxlen], sr\n",
        "\n",
        "        elif len(aud) < maxlen:\n",
        "            pad = maxlen - len(aud)\n",
        "            pad = np.zeros((pad))\n",
        "            return np.concatenate((aud, pad), 0), sr\n",
        "\n",
        "\n",
        "\n",
        "class aud_img:\n",
        "    @staticmethod\n",
        "    def melspec(data, sr):                                                             # returns 3 channels, deplicated from 1\n",
        "        spec = lb.feature.melspectrogram(data, sr=sr, power=1)                         # power = 1/2 changes amplitude_to_db or power_to_db\n",
        "        spec = lb.amplitude_to_db(spec, ref=np.min)\n",
        "        spec = np.expand_dims(spec, axis=2)\n",
        "        return np.stack((spec,)*3, axis=2).squeeze()\n",
        "\n",
        "    @staticmethod\n",
        "    def mfcc(data, sr):                                                                # returns 3 channels, deplicated from 1\n",
        "        mfcc_ = lb.feature.mfcc(data, sr)\n",
        "        #mfcc_ = sk.preprocessing.scale(mfcc_, axis=1)\n",
        "        mfcc_ = np.expand_dims(mfcc_, axis=2)\n",
        "        return np.stack((mfcc_,)*3, axis=2).squeeze()\n",
        "\n",
        "    @staticmethod\n",
        "    def display_audio_img(spec, sr , mfcc=False):\n",
        "        fig, ax = plt.subplots()\n",
        "        \n",
        "        if mfcc:\n",
        "            specshow(spec, sr=sr, x_axis='time')\n",
        "        else:\n",
        "            img = specshow(spec, x_axis='time', y_axis='mel', sr=sr, fmax=8000, ax=ax)\n",
        "            fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
        "\n",
        "\n",
        "# class rav_prep:\n",
        "#     '''\n",
        "#     01 = neutral, \n",
        "#     02 = calm,  -\n",
        "#     03 = happy, \n",
        "#     04 = sad, \n",
        "#     05 = angry, \n",
        "#     06 = fearful, \n",
        "#     07 = disgust,  -\n",
        "#     08 = surprised -\n",
        "#     '''\n",
        "#     @staticmethod\n",
        "#     def correct_data_type(path):\n",
        "#         if (path.split('/')[-1].split('-')[0] == '03') and (path.split('/')[-1].split('-')[1] == '01') and (path.split('/')[-1].split('-')[2] in ['01', '03', '04', '05', '06']):\n",
        "#           return True\n",
        "#         else:\n",
        "#           return False\n",
        "    \n",
        "#     @staticmethod\n",
        "#     def filter(path):\n",
        "#       counter = 0\n",
        "#       for i in glob.glob(path):\n",
        "#         if rav_prep.correct_data_type(i):\n",
        "#           continue\n",
        "#         elif rav_prep.correct_data_type(i) != True:\n",
        "#           sh.move(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/A_removed_files')\n",
        "#           counter += 1\n",
        "#           continue\n",
        "#       print(f'removed {counter} files')\n",
        "\n",
        "    # @staticmethod\n",
        "    # def move_ravdess_colab(path):                                               # colab google drive paths\n",
        "    #   for i in glob.glob(path):\n",
        "    #     if i.split('/')[-1].split('-')[2] == '05':\n",
        "    #       sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/angry')\n",
        "        \n",
        "    #     elif i.split('/')[-1].split('-')[2] == '06':\n",
        "    #       sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/fear')\n",
        "\n",
        "    #     elif i.split('/')[-1].split('-')[2] == '03':\n",
        "    #       sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/happy')\n",
        "\n",
        "    #     elif i.split('/')[-1].split('-')[2] == '01':\n",
        "    #       sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/neutral')\n",
        "\n",
        "    #     elif i.split('/')[-1].split('-')[2] == '04':\n",
        "    #       sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/sad')\n",
        "\n",
        "\n",
        "class ds_create:\n",
        "    \n",
        "    @staticmethod    \n",
        "    def label_from_bpath(bpath):                                                       # probably will not be used\n",
        "        return bpath.decode('utf-8').split('\\\\')[-2]\n",
        "\n",
        "    @staticmethod\n",
        "    def slices_for_onelabel(path, label):                                              \n",
        "        paths = glob.glob(path + label + '/*wav')\n",
        "\n",
        "        labels = [label_to_ind[label]]*len(paths)\n",
        "\n",
        "        return paths , labels\n",
        "\n",
        "    @staticmethod\n",
        "    def dfpremel(path):\n",
        "        data, sr = aud_util.loadaud(path, sr=16000, mono=True)\n",
        "        data, sr = aud_util.pad_trunc(data, sr, max_ms)                                \n",
        "        mel = aud_img.melspec(data, sr)\n",
        "        return mel\n",
        "    \n",
        "    @staticmethod\n",
        "    def dfpremfcc(path):\n",
        "        data, sr = aud_util.loadaud(path, sr=16000, mono=True)\n",
        "        data, sr = aud_util.pad_trunc(data, sr, max_ms)                                \n",
        "        mel = aud_img.mfcc(data, sr)\n",
        "        return mel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu-N6Za66BPG",
        "outputId": "afae6ca1-ab4e-4e90-ceec-5758792f9795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "removed 0 files\n"
          ]
        }
      ],
      "source": [
        "# '''\n",
        "# organising ravdess data \n",
        "# Done once only, by the time you see this cell, it probably was already run, so you can ignore it \n",
        "# as all the revdess files have already been organised into the sub-emotion folder in the google drive, in\n",
        "# /content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data\n",
        "\n",
        "# '''\n",
        "\n",
        "# rav_prep.filter('/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/Altogether/*.wav')\n",
        "# rav_prep.move_ravdess_colab('/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/Altogether/*.wav')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM7Tux-YGMrp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "BUILDING DATASET PIPELINE (both original data and ravdess)\n",
        "\n",
        "'_o' means original data, excluding any extra data\n",
        "\n",
        " - colab, (btw doing this on a windows machine will break completely because of their stupid backward slash)\n",
        "'''\n",
        "\n",
        "angry_o, _0 =   ds_create.slices_for_onelabel(r'Data/NLP Training Dataset/', 'angry')\n",
        "fear_o, _1 =    ds_create.slices_for_onelabel(r'Data/NLP Training Dataset/', 'fear')\n",
        "happy_o, _2 =   ds_create.slices_for_onelabel(r'Data/NLP Training Dataset/', 'happy')\n",
        "neutral_o, _3 = ds_create.slices_for_onelabel(r'Data/NLP Training Dataset/', 'neutral')\n",
        "sad_o, _4 =     ds_create.slices_for_onelabel(r'Data/NLP Training Dataset/', 'sad')\n",
        "\n",
        "angry_r, r_0 =   ds_create.slices_for_onelabel(r'Data/RAVDESS Dataset Sorted/', 'angry')\n",
        "fear_r, r_1 =    ds_create.slices_for_onelabel(r'Data/RAVDESS Dataset Sorted/', 'fear')\n",
        "happy_r, r_2 =   ds_create.slices_for_onelabel(r'Data/RAVDESS Dataset Sorted/', 'happy')\n",
        "neutral_r, r_3 = ds_create.slices_for_onelabel(r'Data/RAVDESS Dataset Sorted/', 'neutral')\n",
        "sad_r, r_4 =     ds_create.slices_for_onelabel(r'Data/RAVDESS Dataset Sorted/', 'sad')\n",
        "\n",
        "\n",
        "slices = angry_o + fear_o + happy_o + neutral_o + sad_o + angry_r + fear_r + happy_r + neutral_r + sad_r\n",
        "labels = _0 + _1 + _2 + _3 + _4 + r_0 + r_1 + r_2 + r_3 + r_4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFG2j4pS3GxJ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Dataframe style\n",
        "\n",
        "using tf.stack later lol\n",
        "'''\n",
        "\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "df['relative_audio_paths'] = slices\n",
        "df['int_labels'] = labels\n",
        "df['1hot_labels'] = list(to_categorical(labels))\n",
        "\n",
        "df['imgs_3c'] = list(map(ds_create.dfpremel, slices))\n",
        "\n",
        "\n",
        "df = sk.utils.shuffle(df)\n",
        "df.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "icL9-cYI3GxJ",
        "outputId": "b23915d7-7d5c-442b-b3ef-cf2c5d7e6d41"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e45ad8a3-3a98-4362-97bd-5646eebbc0f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relative_audio_paths</th>\n",
              "      <th>int_labels</th>\n",
              "      <th>1hot_labels</th>\n",
              "      <th>imgs_3c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
              "      <td>4</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>[[[38.78178342333567, 38.78178342333567, 38.78...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
              "      <td>[[[42.12918, 42.12918, 42.12918], [42.12918, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
              "      <td>2</td>\n",
              "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
              "      <td>[[[72.87353194359729, 72.87353194359729, 72.87...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
              "      <td>4</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>[[[57.963776, 57.963776, 57.963776], [57.84632...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>[[[49.65379750655458, 49.65379750655458, 49.65...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e45ad8a3-3a98-4362-97bd-5646eebbc0f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e45ad8a3-3a98-4362-97bd-5646eebbc0f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e45ad8a3-3a98-4362-97bd-5646eebbc0f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                relative_audio_paths  int_labels  \\\n",
              "0  /content/drive/MyDrive/NLP/NLP Training Datase...           4   \n",
              "1  /content/drive/MyDrive/NLP/NLP Training Datase...           3   \n",
              "2  /content/drive/MyDrive/NLP/NLP Training Datase...           2   \n",
              "3  /content/drive/MyDrive/NLP/NLP Training Datase...           4   \n",
              "4  /content/drive/MyDrive/NLP/NLP Training Datase...           0   \n",
              "\n",
              "                 1hot_labels  \\\n",
              "0  [0.0, 0.0, 0.0, 0.0, 1.0]   \n",
              "1  [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
              "2  [0.0, 0.0, 1.0, 0.0, 0.0]   \n",
              "3  [0.0, 0.0, 0.0, 0.0, 1.0]   \n",
              "4  [1.0, 0.0, 0.0, 0.0, 0.0]   \n",
              "\n",
              "                                             imgs_3c  \n",
              "0  [[[38.78178342333567, 38.78178342333567, 38.78...  \n",
              "1  [[[42.12918, 42.12918, 42.12918], [42.12918, 4...  \n",
              "2  [[[72.87353194359729, 72.87353194359729, 72.87...  \n",
              "3  [[[57.963776, 57.963776, 57.963776], [57.84632...  \n",
              "4  [[[49.65379750655458, 49.65379750655458, 49.65...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3hBgoy43GxK",
        "outputId": "40f1580f-bec7-4ffb-80c6-507d1cc6f6b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128, 126, 3)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.iloc[0,3].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0_tkT0_8SMW"
      },
      "source": [
        "#original baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TlqaMX-3GxL",
        "outputId": "b4b2739d-71d5-4c41-ec94-4dc566005760"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128, 126, 3)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_shape = df.iloc[0,3].shape\n",
        "input_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0bzHjuMOf8T",
        "outputId": "4c17c699-5bbb-4a55-d2ab-6c60c3b71933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 126, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetv2-m (Functiona  (None, None, None, 1280)  53150388 \n",
            " l)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 20480)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               5243136   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 58,427,065\n",
            "Trainable params: 58,135,033\n",
            "Non-trainable params: 292,032\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "m model\n",
        "'''\n",
        "xin = Input(input_shape)\n",
        "\n",
        "prenet = tf.keras.applications.efficientnet_v2.EfficientNetV2M(weights='imagenet', include_top=False)#, input_shape=input_shape)\n",
        "x = prenet(xin)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='swish')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation='swish')(x)\n",
        "\n",
        "\n",
        "xout = Dense(5, activation='softmax')(x)\n",
        "\n",
        "ownm = Model(xin, xout)\n",
        "ownm.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), metrics=['acc', tfa.metrics.F1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
        "ownm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6-Qzo5J3GxL",
        "outputId": "dbf45ca1-9b37-466a-aa2d-2a827710c2b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 126, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetv2-s (Functiona  (None, None, None, 1280)  20331360 \n",
            " l)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 20480)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               5243136   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,608,037\n",
            "Trainable params: 25,454,165\n",
            "Non-trainable params: 153,872\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "s model\n",
        "'''\n",
        "xin = Input(input_shape)\n",
        "\n",
        "prenet = tf.keras.applications.efficientnet_v2.EfficientNetV2S(weights='imagenet', include_top=False)#, input_shape=input_shape)\n",
        "x = prenet(xin)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='swish')(x)\n",
        "x = Dense(128, activation='swish')(x)\n",
        "\n",
        "\n",
        "xout = Dense(5, activation='softmax')(x)\n",
        "\n",
        "own = Model(xin, xout)\n",
        "own.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), metrics=['acc', tfa.metrics.F1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
        "own.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ7XBRJw3GxM"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.1, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint(r'/content/drive/MyDrive/NLP/Zhihaos stuff/effinet_v2m', monitor='val_loss', verbose=0, save_best_only=True)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcgEkPN23GxM"
      },
      "outputs": [],
      "source": [
        "history = ownm.fit(\n",
        "    x=tf.stack(df['imgs_3c']),\n",
        "    y=tf.stack(df['1hot_labels']),\n",
        "    batch_size=batchs,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "ownm.save(r'/content/drive/MyDrive/NLP/Saved Models/modelM_1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5scV_W8n07yj",
        "outputId": "b592f697-0135-48d7-9dbe-3bc97e49204a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-s_notop.h5\n",
            "82427904/82420632 [==============================] - 3s 0us/step\n",
            "82436096/82420632 [==============================] - 3s 0us/step\n",
            "Epoch 1/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 1.3389 - acc: 0.5965 - f1_score: 0.5298INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 138s 2s/step - loss: 1.3389 - acc: 0.5965 - f1_score: 0.5298 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.9738 - acc: 0.8032 - f1_score: 0.7856INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 108s 2s/step - loss: 0.9738 - acc: 0.8032 - f1_score: 0.7856 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.8717 - acc: 0.8728 - f1_score: 0.8654INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 109s 2s/step - loss: 0.8717 - acc: 0.8728 - f1_score: 0.8654 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7913 - acc: 0.9280 - f1_score: 0.9202INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 110s 2s/step - loss: 0.7913 - acc: 0.9280 - f1_score: 0.9202 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7620 - acc: 0.9464 - f1_score: 0.9430INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 108s 2s/step - loss: 0.7620 - acc: 0.9464 - f1_score: 0.9430 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7555 - acc: 0.9494 - f1_score: 0.9455INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 109s 2s/step - loss: 0.7555 - acc: 0.9494 - f1_score: 0.9455 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7454 - acc: 0.9533 - f1_score: 0.9517INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 109s 2s/step - loss: 0.7454 - acc: 0.9533 - f1_score: 0.9517 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.7746 - acc: 0.9324 - f1_score: 0.9323 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7097 - acc: 0.9732 - f1_score: 0.9719INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 108s 2s/step - loss: 0.7097 - acc: 0.9732 - f1_score: 0.9719 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7036 - acc: 0.9782 - f1_score: 0.9765INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 110s 2s/step - loss: 0.7036 - acc: 0.9782 - f1_score: 0.9765 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.7042 - acc: 0.9755 - f1_score: 0.9740 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.6903 - acc: 0.9867 - f1_score: 0.9860INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 109s 2s/step - loss: 0.6903 - acc: 0.9867 - f1_score: 0.9860 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.6916 - acc: 0.9840 - f1_score: 0.9834 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "69/69 [==============================] - 14s 198ms/step - loss: 0.6937 - acc: 0.9821 - f1_score: 0.9825 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.6809 - acc: 0.9892 - f1_score: 0.9886INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 111s 2s/step - loss: 0.6809 - acc: 0.9892 - f1_score: 0.9886 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.6839 - acc: 0.9879 - f1_score: 0.9879 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.6823 - acc: 0.9897 - f1_score: 0.9892 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.6749 - acc: 0.9934 - f1_score: 0.9929INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 109s 2s/step - loss: 0.6749 - acc: 0.9934 - f1_score: 0.9929 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.6882 - acc: 0.9856 - f1_score: 0.9855 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "69/69 [==============================] - 14s 198ms/step - loss: 0.6869 - acc: 0.9888 - f1_score: 0.9856 - lr: 0.0010\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/model2_5/assets\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Ensemble Learning modelm_5 left to be trained\n",
        "'''\n",
        "\n",
        "for count in range(5,6): # already ran 1-3 and saved models\n",
        "    \n",
        "    xin = Input(input_shape)\n",
        "\n",
        "    prenet = tf.keras.applications.efficientnet_v2.EfficientNetV2S(weights='imagenet', include_top=False)#, input_shape=input_shape)\n",
        "    x = prenet(xin)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(256, activation='swish')(x)\n",
        "    x = Dense(128, activation='swish')(x)\n",
        "\n",
        "\n",
        "    xout = Dense(5, activation='softmax')(x)\n",
        "\n",
        "    own = Model(xin, xout)\n",
        "    own.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), metrics=['acc', tfa.metrics.F1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
        "    \n",
        "    callbacks_en = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=3, factor=0.1, verbose=1),\n",
        "    # tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint(r'/content/drive/MyDrive/NLP/Saved Models/Model Checkpoints', monitor='loss', verbose=0, save_best_only=True)\n",
        "    ]\n",
        "\n",
        "\n",
        "    history = own.fit(\n",
        "        x=tf.stack(df['imgs_3c']),\n",
        "        y=tf.stack(df['1hot_labels']),\n",
        "        batch_size=batchs,\n",
        "        epochs=epochs, \n",
        "        #validation_split=0.2,\n",
        "        callbacks=callbacks_en,\n",
        "    )   \n",
        "    own.save(r'/content/drive/MyDrive/NLP/Saved Models/model2_' + str(count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwv-_DMS1kbr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Ensemble Learning effinet v2s\n",
        "'''\n",
        "model_1 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model_1') # val_f1_score is 0.8869\n",
        "model_2 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model_2') # 0.8894\n",
        "model_3 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model_3') # 0.8956\n",
        "model_4 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model_4') # 0.8736\n",
        "model_5 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model_5') # 0.8845"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uZ6HyPmCc2w"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Ensemble Learning effinet v2s no val_split\n",
        "'''\n",
        "modelm_1 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model2_1')\n",
        "modelm_2 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model2_2')\n",
        "modelm_3 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model2_3') \n",
        "modelm_4 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model2_4')\n",
        "modelm_5 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model2_5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qadd6XzL8E8R"
      },
      "source": [
        "# Generating the test preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tFlMT1x3GxM"
      },
      "outputs": [],
      "source": [
        "tst = ds_create.dfpremel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/fear/00530e07e3.wav')\n",
        "tst = ds_create.dup_channel(tst)\n",
        "tst = np.expand_dims(tst, axis=0)                                             # EXPAND DIMS OF FIRST DIMENSION ARGHHHHHH\n",
        "pred = own.predict(tst)\n",
        "pred = np.argmax(pred)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6goTowRgPN4",
        "outputId": "92d93c86-29f0-44e1-d24d-24df680bf05c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1eebd177d0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "own.load_weights(r'/content/drive/MyDrive/NLP/Zhihaos stuff/effinet_v2s_nodrop')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClBa4ZF3fEIN",
        "outputId": "f03c43b5-52a8-4759-bf1e-48c082c632fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "137/137 [==============================] - 20s 47ms/step - loss: 0.6959 - acc: 0.9794 - f1_score: 0.9794\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.6958815455436707, 0.9793767333030701, 0.9794116616249084]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "own.evaluate(\n",
        "    x=tf.stack(df['imgs_3c']),\n",
        "    y=tf.stack(df['1hot_labels'])\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDMxvMdP3GxN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Generating the qualifying csv file\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "class test_gen:\n",
        "    @staticmethod\n",
        "    def path_gen(path):                                              #for zhihao's local pc\n",
        "        paths = os.listdir(path)\n",
        "        paths = list(map(lambda x : 'DATA_NLP_TIL\\\\.qualifying_test\\\\'+x , paths))\n",
        "\n",
        "        return paths \n",
        "\n",
        "    @staticmethod\n",
        "    def path_gen_colab(path):                                              #for colab, zhihaos\n",
        "        paths = os.listdir(path)\n",
        "        paths = list(map(lambda x : '/content/drive/MyDrive/NLP/NLP Interim Dataset/NLP/'+x , paths))\n",
        "\n",
        "        return paths \n",
        "\n",
        "    @staticmethod\n",
        "    def path_to_mel(path):\n",
        "        ccc = ds_create.dfpremel(path)\n",
        "        return ccc\n",
        "    \n",
        "    @staticmethod\n",
        "    def path_to_mfcc(path):\n",
        "        ccc = ds_create.dfpremfcc(path)\n",
        "        return ccc\n",
        "\n",
        "    @staticmethod\n",
        "    def int_to_label(int):\n",
        "        return ind_to_label[int]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QJXAxyVMp7y"
      },
      "outputs": [],
      "source": [
        "q_df = pd.DataFrame()\n",
        "paths = sorted(glob.glob(r'/content/drive/MyDrive/NLP/NLP Interim Dataset/NLP/*.wav'))\n",
        "q_data = list(map(test_gen.path_to_mel, paths))\n",
        "\n",
        "q_data = tf.stack(q_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYh2V6ZkBA22"
      },
      "outputs": [],
      "source": [
        "# preds = own.predict(q_data)\n",
        "# preds = np.argmax(preds, axis=1)\n",
        "'''\n",
        "Ensemble Learning\n",
        "'''\n",
        "pred_1 = modelm_1.predict(q_data)\n",
        "pred_2 = modelm_2.predict(q_data)\n",
        "pred_3 = modelm_3.predict(q_data)\n",
        "pred_4 = modelm_4.predict(q_data)\n",
        "pred_5 = modelm_5.predict(q_data)\n",
        "\n",
        "pred_comb = pred_1 * 0.2 + pred_2 * 0.2 + pred_3 * 0.2 + pred_4 * 0.2 + pred_5 * 0.2\n",
        "pred_comb = np.argmax(pred_comb, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFypbX_xvmLn",
        "outputId": "a8f9877b-234e-49ab-8124-e805e7d6cfc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(600,)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMiSafBkOTfp"
      },
      "outputs": [],
      "source": [
        "q_df['paths'] = sorted(os.listdir(r'/content/drive/MyDrive/NLP/NLP Interim Dataset/NLP/'))\n",
        "q_df['labels'] = list(map(\n",
        "    test_gen.int_to_label,\n",
        "    list(pred_comb)\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "rbFehkxqUGss",
        "outputId": "c9172d9e-28a1-4a9b-f5ce-127ccfbc7dd1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b4c5f51a-f209-4aa2-89ae-58152e0b64cf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paths</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00ae09ba94.wav</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00f2a00f1f.wav</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>012822b908.wav</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0144091c26.wav</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0145cb0279.wav</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4c5f51a-f209-4aa2-89ae-58152e0b64cf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4c5f51a-f209-4aa2-89ae-58152e0b64cf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4c5f51a-f209-4aa2-89ae-58152e0b64cf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            paths labels\n",
              "0  00ae09ba94.wav  happy\n",
              "1  00f2a00f1f.wav  angry\n",
              "2  012822b908.wav   fear\n",
              "3  0144091c26.wav    sad\n",
              "4  0145cb0279.wav    sad"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcxfIZE2UlkK"
      },
      "outputs": [],
      "source": [
        "q_df.to_csv(r'/content/drive/MyDrive/NLP/Zhihao nlp preds/qualifiers_ensemble3.csv', header=False, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copy of zhihao_nlp_2022TIL_(ignore othernotebook).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "38c7ce74dd526bc9e84fd0682f6c1ac8fcd6c4cb0e87d36fcf4e0e214217cc07"
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
=======
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "pip install tensorflow_addons"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNub-g9OTBer",
    "outputId": "edf87fc4-6423-400b-be8c-c4d690561c3d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8g4N2Je3GxB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil as sh\n",
    "import random as rng\n",
    "import glob\n",
    "# import itertools\n",
    "\n",
    "import librosa as lb\n",
    "from librosa.display import specshow\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9emmQj03Rsx",
    "outputId": "bb6e3405-4019-4548-c6ce-3582ba1ac8c0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iICR-Ysk3GxE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  --  Defining Variables  --  #\n",
    "\n",
    "max_ms = 4000\n",
    "\n",
    "batchs = 128\n",
    "epochs = 100\n",
    "\n",
    "ind_to_label = {\n",
    "    0 : 'angry',\n",
    "    1 : 'fear',\n",
    "    2 : 'happy',\n",
    "    3 : 'neutral',\n",
    "    4 : 'sad'\n",
    "}\n",
    "\n",
    "label_to_ind = { \n",
    "    lab: ind for ind, lab in ind_to_label.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRm58mWP3GxE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Data insights\n",
    "\n",
    "'''\n",
    "\n",
    "class aud_stats:\n",
    "    @staticmethod\n",
    "    def average_sr():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "\n",
    "PREPROCESSING UTILS\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class aud_util:\n",
    "    \n",
    "    @staticmethod\n",
    "    def loadaud(audio_file_path, sr=None, mono=False):                                 # load audio file, *mono argument (bool) can auto convert to mono, while default sr is converted to 22050*\n",
    "        return lb.load(audio_file_path, sr=sr, mono=mono)                              # returns (data, sr)       \n",
    "\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, sr, target_ms):                                                 # padding places shorter audio randomly within the time frame of the padded length\n",
    "        maxlen = (target_ms//1000)*sr\n",
    "        \n",
    "        if len(aud) == maxlen:\n",
    "            return aud, sr\n",
    "\n",
    "        elif len(aud) > maxlen:\n",
    "            return aud[:maxlen], sr\n",
    "\n",
    "        elif len(aud) < maxlen:\n",
    "            pad = maxlen - len(aud)\n",
    "            pad = np.zeros((pad))\n",
    "            return np.concatenate((aud, pad), 0), sr\n",
    "\n",
    "\n",
    "\n",
    "class aud_img:\n",
    "    @staticmethod\n",
    "    def melspec(data, sr):                                                             # returns 3 channels, deplicated from 1\n",
    "        spec = lb.feature.melspectrogram(data, sr=sr, power=1)                         # power = 1/2 changes amplitude_to_db or power_to_db\n",
    "        spec = lb.amplitude_to_db(spec, ref=np.min)\n",
    "        spec = np.expand_dims(spec, axis=2)\n",
    "        return np.stack((spec,)*3, axis=2).squeeze()\n",
    "\n",
    "    @staticmethod\n",
    "    def mfcc(data, sr):                                                                # returns 3 channels, deplicated from 1\n",
    "        mfcc_ = lb.feature.mfcc(data, sr)\n",
    "        #mfcc_ = sk.preprocessing.scale(mfcc_, axis=1)\n",
    "        mfcc_ = np.expand_dims(mfcc_, axis=2)\n",
    "        return np.stack((mfcc_,)*3, axis=2).squeeze()\n",
    "\n",
    "    @staticmethod\n",
    "    def display_audio_img(spec, sr , mfcc=False):\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "        if mfcc:\n",
    "            specshow(spec, sr=sr, x_axis='time')\n",
    "        else:\n",
    "            img = specshow(spec, x_axis='time', y_axis='mel', sr=sr, fmax=8000, ax=ax)\n",
    "            fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "\n",
    "\n",
    "class rav_prep:\n",
    "    '''\n",
    "    01 = neutral, \n",
    "    02 = calm,  -\n",
    "    03 = happy, \n",
    "    04 = sad, \n",
    "    05 = angry, \n",
    "    06 = fearful, \n",
    "    07 = disgust,  -\n",
    "    08 = surprised -\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def correct_data_type(path):\n",
    "        if (path.split('/')[-1].split('-')[0] == '03') and (path.split('/')[-1].split('-')[1] == '01') and (path.split('/')[-1].split('-')[2] in ['01', '03', '04', '05', '06']):\n",
    "          return True\n",
    "        else:\n",
    "          return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter(path):\n",
    "      counter = 0\n",
    "      for i in glob.glob(path):\n",
    "        if rav_prep.correct_data_type(i):\n",
    "          continue\n",
    "        elif rav_prep.correct_data_type(i) != True:\n",
    "          sh.move(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/A_removed_files')\n",
    "          counter += 1\n",
    "          continue\n",
    "      print(f'removed {counter} files')\n",
    "\n",
    "    @staticmethod\n",
    "    def move_ravdess_colab(path):                                               # colab google drive paths\n",
    "      for i in glob.glob(path):\n",
    "        if i.split('/')[-1].split('-')[2] == '05':\n",
    "          sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/angry')\n",
    "        \n",
    "        elif i.split('/')[-1].split('-')[2] == '06':\n",
    "          sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/fear')\n",
    "\n",
    "        elif i.split('/')[-1].split('-')[2] == '03':\n",
    "          sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/happy')\n",
    "\n",
    "        elif i.split('/')[-1].split('-')[2] == '01':\n",
    "          sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/neutral')\n",
    "\n",
    "        elif i.split('/')[-1].split('-')[2] == '04':\n",
    "          sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/sad')\n",
    "\n",
    "\n",
    "class ds_create:\n",
    "    \n",
    "    @staticmethod    \n",
    "    def label_from_bpath(bpath):                                                       # probably will not be used\n",
    "        return bpath.decode('utf-8').split('\\\\')[-2]\n",
    "\n",
    "    @staticmethod\n",
    "    def slices_for_onelabel(path, label):                                              \n",
    "        paths = glob.glob(path)\n",
    "\n",
    "        labels = [label_to_ind[label]]*len(paths)\n",
    "\n",
    "        return paths , labels\n",
    "\n",
    "    @staticmethod\n",
    "    def dfpremel(path):\n",
    "        data, sr = aud_util.loadaud(path, sr=16000, mono=True)\n",
    "        data, sr = aud_util.pad_trunc(data, sr, max_ms)                                \n",
    "        mel = aud_img.melspec(data, sr)\n",
    "        return mel\n",
    "    \n",
    "    @staticmethod\n",
    "    def dfpremfcc(path):\n",
    "        data, sr = aud_util.loadaud(path, sr=16000, mono=True)\n",
    "        data, sr = aud_util.pad_trunc(data, sr, max_ms)                                \n",
    "        mel = aud_img.mfcc(data, sr)\n",
    "        return mel\n",
    "\n"
   ],
   "metadata": {
    "id": "ITKWjwvvnZnk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "organising ravdess data \n",
    "Done once only, by the time you see this cell, it probably was already run, so you can ignore it \n",
    "as all the revdess files have already been organised into the sub-emotion folder in the google drive, in\n",
    "/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data\n",
    "\n",
    "'''\n",
    "\n",
    "rav_prep.filter('/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/Altogether/*.wav')\n",
    "rav_prep.move_ravdess_colab('/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/Altogether/*.wav')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uu-N6Za66BPG",
    "outputId": "9722e1d4-a127-4eea-d008-dacce07e20c5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "removed 576 files\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCQeMCrk3GxI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "BUILDING DATASET PIPELINE (only original training data included)\n",
    "\n",
    "'_o' means original data, excluding any extra data\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "angry_o, _0 =   ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/angry/*.wav', 'angry')\n",
    "fear_o, _1 =    ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/fear/*.wav', 'fear')\n",
    "happy_o, _2 =   ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/happy/*.wav', 'happy')\n",
    "neutral_o, _3 = ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/neutral/*.wav', 'neutral')\n",
    "sad_o, _4 =     ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/sad/*.wav', 'sad')\n",
    "\n",
    "slices = angry_o + fear_o + happy_o + neutral_o + sad_o\n",
    "labels = _0 + _1 + _2 + _3 + _4\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "BUILDING DATASET PIPELINE (both original data and ravdess)\n",
    "\n",
    "'_o' means original data, excluding any extra data\n",
    "\n",
    "'''\n",
    "\n",
    "angry_o, _0 =   ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/angry/*.wav', 'angry')\n",
    "fear_o, _1 =    ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/fear/*.wav', 'fear')\n",
    "happy_o, _2 =   ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/happy/*.wav', 'happy')\n",
    "neutral_o, _3 = ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/neutral/*.wav', 'neutral')\n",
    "sad_o, _4 =     ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/sad/*.wav', 'sad')\n",
    "\n",
    "angry_r, r_0 =   ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/angry/*.wav', 'angry')\n",
    "fear_r, r_1 =    ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/fear/*.wav', 'fear')\n",
    "happy_r, r_2 =   ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/happy/*.wav', 'happy')\n",
    "neutral_r, r_3 = ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/neutral/*.wav', 'neutral')\n",
    "sad_r, r_4 =     ds_create.slices_for_onelabel(r'/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/sad/*.wav', 'sad')\n",
    "\n",
    "\n",
    "slices = angry_o + fear_o + happy_o + neutral_o + sad_o    +   angry_r + fear_r + happy_r + neutral_r + sad_r\n",
    "labels = _0 + _1 + _2 + _3 + _4   +   r_0 + r_1 + r_2 + r_3 + r_4\n",
    "\n"
   ],
   "metadata": {
    "id": "HM7Tux-YGMrp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFG2j4pS3GxJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Dataframe style\n",
    "\n",
    "using tf.stack later lol\n",
    "'''\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['relative_audio_paths'] = slices\n",
    "df['int_labels'] = labels\n",
    "df['1hot_labels'] = list(to_categorical(labels))\n",
    "\n",
    "df['imgs_3c'] = list(map(ds_create.dfpremel, slices))\n",
    "\n",
    "\n",
    "df = sk.utils.shuffle(df)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "icL9-cYI3GxJ",
    "outputId": "a0486113-af97-4f25-fc93-47283bb1d432",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                relative_audio_paths  int_labels  \\\n",
       "0  /content/drive/MyDrive/NLP/RAVDESS_altogether/...           1   \n",
       "1  /content/drive/MyDrive/NLP/NLP Training Datase...           2   \n",
       "2  /content/drive/MyDrive/NLP/NLP Training Datase...           4   \n",
       "3  /content/drive/MyDrive/NLP/RAVDESS_altogether/...           1   \n",
       "4  /content/drive/MyDrive/NLP/NLP Training Datase...           4   \n",
       "\n",
       "                 1hot_labels  \\\n",
       "0  [0.0, 1.0, 0.0, 0.0, 0.0]   \n",
       "1  [0.0, 0.0, 1.0, 0.0, 0.0]   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "3  [0.0, 1.0, 0.0, 0.0, 0.0]   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 1.0]   \n",
       "\n",
       "                                             imgs_3c  \n",
       "0  [[[37.347054, 37.347054, 37.347054], [37.34705...  \n",
       "1  [[[63.72838126616919, 63.72838126616919, 63.72...  \n",
       "2  [[[87.36988820126425, 87.36988820126425, 87.36...  \n",
       "3  [[[38.92079559238242, 38.92079559238242, 38.92...  \n",
       "4  [[[76.25816571916035, 76.25816571916035, 76.25...  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-687c041f-9e07-4fd0-857e-7872a1e0e9ee\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_audio_paths</th>\n",
       "      <th>int_labels</th>\n",
       "      <th>1hot_labels</th>\n",
       "      <th>imgs_3c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/content/drive/MyDrive/NLP/RAVDESS_altogether/...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[[37.347054, 37.347054, 37.347054], [37.34705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[[[63.72838126616919, 63.72838126616919, 63.72...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[[[87.36988820126425, 87.36988820126425, 87.36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/content/drive/MyDrive/NLP/RAVDESS_altogether/...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[[38.92079559238242, 38.92079559238242, 38.92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "      <td>[[[76.25816571916035, 76.25816571916035, 76.25...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-687c041f-9e07-4fd0-857e-7872a1e0e9ee')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-687c041f-9e07-4fd0-857e-7872a1e0e9ee button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-687c041f-9e07-4fd0-857e-7872a1e0e9ee');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3hBgoy43GxK",
    "outputId": "5b7d2867-4b45-44db-8623-448775f1b09c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(128, 126, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "df.iloc[0,3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SW3giI733GxK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ignore this cell\n",
    "\n",
    "'''\n",
    "# requires shuffling in tf.keras.Model.fit\n",
    "\n",
    "X = list(map(ds_create.path_to_mel, slices))\n",
    "\n",
    "Y = np.array(to_categorical(labels))\n",
    "X = np.array(X)\n",
    "\n",
    "\n",
    "#ds = ds.map(ds_create.preprocess_mel_eachlabel)\n",
    "ds = ds.map(ds_create.preprocess_mel_eachlabel)\n",
    "# ds = ds.cache()\n",
    "# ds = ds.batch(batchs)\n",
    "# ds = ds.prefetch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TlqaMX-3GxL",
    "outputId": "90f95f00-d646-42fd-87e9-d890aef265bc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(128, 126, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "input_shape = df.iloc[0,3].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6-Qzo5J3GxL",
    "outputId": "8b341ddc-7d97-4179-ce98-44274b867fe7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 126, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetv2-s (Functiona  (None, None, None, 1280)  20331360 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20480)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               5243136   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,608,037\n",
      "Trainable params: 25,454,165\n",
      "Non-trainable params: 153,872\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "xin = Input(input_shape)\n",
    "\n",
    "prenet = tf.keras.applications.efficientnet_v2.EfficientNetV2S(weights='imagenet', include_top=False)#, input_shape=input_shape)\n",
    "x = prenet(xin)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='swish')(x)\n",
    "x = Dropout(0.65)(x)\n",
    "x = Dense(128, activation='swish')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "xout = Dense(5, activation='softmax')(x)\n",
    "\n",
    "own = Model(xin, xout)\n",
    "own.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['acc', tfa.metrics.F1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
    "own.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQ7XBRJw3GxM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.1, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(r'/content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s', monitor='val_loss', verbose=0, save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcgEkPN23GxM",
    "outputId": "b18f9536-dfc0-4939-e20a-dae8e2cc8f6c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.5855 - acc: 0.3710 - f1_score: 0.2792INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s/assets\n",
      "28/28 [==============================] - 130s 4s/step - loss: 1.5855 - acc: 0.3710 - f1_score: 0.2792 - val_loss: 1.1825 - val_acc: 0.5464 - val_f1_score: 0.3523 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 10s 367ms/step - loss: 1.0994 - acc: 0.5783 - f1_score: 0.5257 - val_loss: 1.1956 - val_acc: 0.6025 - val_f1_score: 0.5603 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8599 - acc: 0.6772 - f1_score: 0.6622INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s/assets\n",
      "28/28 [==============================] - 107s 4s/step - loss: 0.8599 - acc: 0.6772 - f1_score: 0.6622 - val_loss: 0.7853 - val_acc: 0.7251 - val_f1_score: 0.7014 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 10s 368ms/step - loss: 0.8243 - acc: 0.7178 - f1_score: 0.6932 - val_loss: 0.8167 - val_acc: 0.6930 - val_f1_score: 0.6355 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 10s 367ms/step - loss: 0.7875 - acc: 0.7210 - f1_score: 0.6983 - val_loss: 0.8913 - val_acc: 0.6930 - val_f1_score: 0.6783 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7460 - acc: 0.7267 - f1_score: 0.7081INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s/assets\n",
      "28/28 [==============================] - 106s 4s/step - loss: 0.7460 - acc: 0.7267 - f1_score: 0.7081 - val_loss: 0.7052 - val_acc: 0.7549 - val_f1_score: 0.7358 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6826 - acc: 0.7557 - f1_score: 0.7381INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s/assets\n",
      "28/28 [==============================] - 108s 4s/step - loss: 0.6826 - acc: 0.7557 - f1_score: 0.7381 - val_loss: 0.6920 - val_acc: 0.7881 - val_f1_score: 0.7867 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 10s 369ms/step - loss: 0.5584 - acc: 0.7983 - f1_score: 0.7876 - val_loss: 1.1029 - val_acc: 0.7537 - val_f1_score: 0.7646 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 10s 371ms/step - loss: 0.8040 - acc: 0.7279 - f1_score: 0.6961 - val_loss: 0.7463 - val_acc: 0.7549 - val_f1_score: 0.7442 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6655 - acc: 0.7892 - f1_score: 0.7610INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s/assets\n",
      "28/28 [==============================] - 107s 4s/step - loss: 0.6655 - acc: 0.7892 - f1_score: 0.7610 - val_loss: 0.6865 - val_acc: 0.7572 - val_f1_score: 0.7474 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4929 - acc: 0.8330 - f1_score: 0.8225INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s/assets\n",
      "28/28 [==============================] - 107s 4s/step - loss: 0.4929 - acc: 0.8330 - f1_score: 0.8225 - val_loss: 0.6242 - val_acc: 0.7824 - val_f1_score: 0.7908 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 10s 368ms/step - loss: 0.3248 - acc: 0.9003 - f1_score: 0.8914 - val_loss: 0.6752 - val_acc: 0.8110 - val_f1_score: 0.8123 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 10s 368ms/step - loss: 0.3744 - acc: 0.8954 - f1_score: 0.8842 - val_loss: 0.7363 - val_acc: 0.7423 - val_f1_score: 0.7515 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3681 - acc: 0.8963 - f1_score: 0.8858INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s/assets\n",
      "28/28 [==============================] - 109s 4s/step - loss: 0.3681 - acc: 0.8963 - f1_score: 0.8858 - val_loss: 0.5675 - val_acc: 0.8259 - val_f1_score: 0.8245 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2736 - acc: 0.9161 - f1_score: 0.9139INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s/assets\n",
      "28/28 [==============================] - 107s 4s/step - loss: 0.2736 - acc: 0.9161 - f1_score: 0.9139 - val_loss: 0.5310 - val_acc: 0.8477 - val_f1_score: 0.8411 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 10s 369ms/step - loss: 0.2067 - acc: 0.9453 - f1_score: 0.9447 - val_loss: 0.8518 - val_acc: 0.8076 - val_f1_score: 0.8057 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 10s 368ms/step - loss: 0.4142 - acc: 0.8702 - f1_score: 0.8680 - val_loss: 0.6478 - val_acc: 0.8156 - val_f1_score: 0.8136 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3019 - acc: 0.9161 - f1_score: 0.9142\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "28/28 [==============================] - 10s 369ms/step - loss: 0.3019 - acc: 0.9161 - f1_score: 0.9142 - val_loss: 0.5416 - val_acc: 0.8442 - val_f1_score: 0.8466 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1620 - acc: 0.9530 - f1_score: 0.9514INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s/assets\n",
      "28/28 [==============================] - 108s 4s/step - loss: 0.1620 - acc: 0.9530 - f1_score: 0.9514 - val_loss: 0.4487 - val_acc: 0.8660 - val_f1_score: 0.8717 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0972 - acc: 0.9765 - f1_score: 0.9745INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Zhihaos stuff/effinet v2s/assets\n",
      "28/28 [==============================] - 108s 4s/step - loss: 0.0972 - acc: 0.9765 - f1_score: 0.9745 - val_loss: 0.4420 - val_acc: 0.8774 - val_f1_score: 0.8808 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 10s 369ms/step - loss: 0.0753 - acc: 0.9791 - f1_score: 0.9767 - val_loss: 0.4698 - val_acc: 0.8843 - val_f1_score: 0.8888 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 10s 368ms/step - loss: 0.0645 - acc: 0.9814 - f1_score: 0.9802 - val_loss: 0.4619 - val_acc: 0.8809 - val_f1_score: 0.8829 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0494 - acc: 0.9860 - f1_score: 0.9851\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "28/28 [==============================] - 10s 367ms/step - loss: 0.0494 - acc: 0.9860 - f1_score: 0.9851 - val_loss: 0.4914 - val_acc: 0.8774 - val_f1_score: 0.8828 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 10s 368ms/step - loss: 0.0459 - acc: 0.9865 - f1_score: 0.9864 - val_loss: 0.4944 - val_acc: 0.8797 - val_f1_score: 0.8841 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 10s 368ms/step - loss: 0.0534 - acc: 0.9891 - f1_score: 0.9883 - val_loss: 0.5092 - val_acc: 0.8809 - val_f1_score: 0.8827 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0512 - acc: 0.9877 - f1_score: 0.9831\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "28/28 [==============================] - 10s 368ms/step - loss: 0.0512 - acc: 0.9877 - f1_score: 0.9831 - val_loss: 0.4986 - val_acc: 0.8786 - val_f1_score: 0.8843 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 11s 377ms/step - loss: 0.0442 - acc: 0.9888 - f1_score: 0.9868 - val_loss: 0.4999 - val_acc: 0.8832 - val_f1_score: 0.8839 - lr: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "history = own.fit(\n",
    "    x=tf.stack(df['imgs_3c']),\n",
    "    y=tf.stack(df['1hot_labels']),\n",
    "    batch_size=batchs,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tFlMT1x3GxM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tst = ds_create.dfpremel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/fear/00530e07e3.wav')\n",
    "tst = ds_create.dup_channel(tst)\n",
    "tst = np.expand_dims(tst, axis=0)                                             # EXPAND DIMS OF FIRST DIMENSION ARGHHHHHH\n",
    "pred = own.predict(tst)\n",
    "pred = np.argmax(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDMxvMdP3GxN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generating the qualifying csv file\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class test_gen:\n",
    "    @staticmethod\n",
    "    def path_gen(path):                                              #for zhihao's local pc\n",
    "        paths = os.listdir(path)\n",
    "        paths = list(map(lambda x : 'DATA_NLP_TIL\\\\.qualifying_test\\\\'+x , paths))\n",
    "\n",
    "        return paths \n",
    "\n",
    "    @staticmethod\n",
    "    def path_gen_colab(path):                                              #for colab, zhihaos\n",
    "        paths = os.listdir(path)\n",
    "        paths = list(map(lambda x : '/content/drive/MyDrive/NLP/NLP Interim Dataset/NLP/'+x , paths))\n",
    "\n",
    "        return paths \n",
    "\n",
    "    @staticmethod\n",
    "    def path_to_mel(path):\n",
    "        ccc = ds_create.dfpremel(path)\n",
    "        return ccc\n",
    "    \n",
    "    @staticmethod\n",
    "    def path_to_mfcc(path):\n",
    "        ccc = ds_create.dfpremfcc(path)\n",
    "        return ccc\n",
    "\n",
    "    @staticmethod\n",
    "    def int_to_label(int):\n",
    "        return ind_to_label[int]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "q_df = pd.DataFrame()\n",
    "paths = sorted(glob.glob(r'/content/drive/MyDrive/NLP/NLP Interim Dataset/NLP/*.wav'))\n",
    "q_data = list(map(test_gen.path_to_mel, paths))\n",
    "\n",
    "q_data = tf.stack(q_data)\n",
    "\n",
    "preds = own.predict(q_data)\n",
    "preds = np.argmax(preds, axis=1)"
   ],
   "metadata": {
    "id": "2QJXAxyVMp7y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q_df['paths'] = sorted(os.listdir(r'/content/drive/MyDrive/NLP/NLP Interim Dataset/NLP/'))\n",
    "q_df['labels'] = list(map(\n",
    "    test_gen.int_to_label,\n",
    "    list(preds)\n",
    "))"
   ],
   "metadata": {
    "id": "HMiSafBkOTfp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q_df.head()"
   ],
   "metadata": {
    "id": "rbFehkxqUGss",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "2d519aea-2bd3-4d53-c5c0-c9474216aab3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            paths labels\n",
       "0  00ae09ba94.wav  angry\n",
       "1  00f2a00f1f.wav  angry\n",
       "2  012822b908.wav   fear\n",
       "3  0144091c26.wav    sad\n",
       "4  0145cb0279.wav    sad"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-039a061f-1554-4ebc-8e2c-d18270b248a3\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00ae09ba94.wav</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00f2a00f1f.wav</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>012822b908.wav</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0144091c26.wav</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0145cb0279.wav</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-039a061f-1554-4ebc-8e2c-d18270b248a3')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-039a061f-1554-4ebc-8e2c-d18270b248a3 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-039a061f-1554-4ebc-8e2c-d18270b248a3');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "q_df.to_csv(r'/content/drive/MyDrive/NLP/Zhihao nlp preds/qualifiers4.csv', header=False, index=False)"
   ],
   "metadata": {
    "id": "kcxfIZE2UlkK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38c7ce74dd526bc9e84fd0682f6c1ac8fcd6c4cb0e87d36fcf4e0e214217cc07"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "colab": {
   "name": "zhihao_nlp_2022TIL_(ignore othernotebook).ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
>>>>>>> 4de7aef52f72f5764744713333905c181e6f7b12:NLP/zhihao_nlp_2022_TIL_with ravdess.ipynb
