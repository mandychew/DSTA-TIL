{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfTT2tGszAvC"
      },
      "source": [
        "# AGENDA\n",
        "- [ ] Try out other ppretrained models other than effinet\n",
        "- [ ] Experiment with using less dropout on larger models\n",
        "- [ ] data augmentation... and loading training images into google drive (after augmenttation). Augment audio(stretch, loudness, noise) and images(vertical, horizontal bars\n",
        "- [ ] Possibly look into MFCCs again  \n",
        "- [x] model ensembling -- done by Mandy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNub-g9OTBer",
        "outputId": "27327787-1e04-4025-ed45-2b4dd1e14261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.0\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I8g4N2Je3GxB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil as sh\n",
        "import random as rng\n",
        "import glob\n",
        "# import itertools\n",
        "\n",
        "import librosa as lb\n",
        "from librosa.display import specshow\n",
        "import IPython.display as ipd\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "import sklearn as sk\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "n9emmQj03Rsx",
        "outputId": "4cdd23ab-85d6-4f98-86b1-a0e41c9c6e9f"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdIDRxQnaL_K"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def map_to_array(example):\n",
        "    speech, _ = librosa.load(example[\"file\"], sr=16000, mono=True)\n",
        "    example[\"speech\"] = speech\n",
        "    return example\n",
        "\n",
        "# load a demo dataset and read audio files\n",
        "dataset = load_dataset(\"anton-l/superb_demo\", \"er\", split=\"session1\")\n",
        "dataset = dataset.map(map_to_array)\n",
        "\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
        "\n",
        "# compute attention masks and normalize the waveform if needed\n",
        "inputs = feature_extractor(dataset[:4][\"speech\"], sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "logits = model(**inputs).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "labels = [model.config.id2label[_id] for _id in predicted_ids.tolist()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iICR-Ysk3GxE"
      },
      "outputs": [],
      "source": [
        "#  --  Defining Variables  --  #\n",
        "\n",
        "max_ms = 4000\n",
        "\n",
        "batchs = 64\n",
        "epochs = 20\n",
        "\n",
        "ind_to_label = {\n",
        "    0 : 'angry',\n",
        "    1 : 'fear',\n",
        "    2 : 'happy',\n",
        "    3 : 'neutral',\n",
        "    4 : 'sad'\n",
        "}\n",
        "\n",
        "label_to_ind = { \n",
        "    lab: ind for ind, lab in ind_to_label.items()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRm58mWP3GxE"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Data insights\n",
        "\n",
        "'''\n",
        "\n",
        "class aud_stats:\n",
        "    @staticmethod\n",
        "    def average_sr():\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITKWjwvvnZnk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "PREPROCESSING UTILS\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "class aud_util:\n",
        "    \n",
        "    @staticmethod\n",
        "    def loadaud(audio_file_path, sr=None, mono=False):                                 # load audio file, *mono argument (bool) can auto convert to mono, while default sr is converted to 22050*\n",
        "        return lb.load(audio_file_path, sr=sr, mono=mono)                              # returns (data, sr)       \n",
        "\n",
        "    @staticmethod\n",
        "    def pad_trunc(aud, sr, target_ms):                                                 # padding places shorter audio randomly within the time frame of the padded length\n",
        "        maxlen = (target_ms//1000)*sr\n",
        "        \n",
        "        if len(aud) == maxlen:\n",
        "            return aud, sr\n",
        "\n",
        "        elif len(aud) > maxlen:\n",
        "            return aud[:maxlen], sr\n",
        "\n",
        "        elif len(aud) < maxlen:\n",
        "            pad = maxlen - len(aud)\n",
        "            pad = np.zeros((pad))\n",
        "            return np.concatenate((aud, pad), 0), sr\n",
        "\n",
        "\n",
        "\n",
        "class aud_img:\n",
        "    @staticmethod\n",
        "    def melspec(data, sr):                                                             # returns 3 channels, deplicated from 1\n",
        "        spec = lb.feature.melspectrogram(data, sr=sr, power=1)                         # power = 1/2 changes amplitude_to_db or power_to_db\n",
        "        spec = lb.amplitude_to_db(spec, ref=np.min)\n",
        "        spec = np.expand_dims(spec, axis=2)\n",
        "        return np.stack((spec,)*3, axis=2).squeeze()\n",
        "\n",
        "    @staticmethod\n",
        "    def mfcc(data, sr):                                                                # returns 3 channels, deplicated from 1\n",
        "        mfcc_ = lb.feature.mfcc(data, sr)\n",
        "        #mfcc_ = sk.preprocessing.scale(mfcc_, axis=1)\n",
        "        mfcc_ = np.expand_dims(mfcc_, axis=2)\n",
        "        return np.stack((mfcc_,)*3, axis=2).squeeze()\n",
        "\n",
        "    @staticmethod\n",
        "    def display_audio_img(spec, sr , mfcc=False):\n",
        "        fig, ax = plt.subplots()\n",
        "        \n",
        "        if mfcc:\n",
        "            specshow(spec, sr=sr, x_axis='time')\n",
        "        else:\n",
        "            img = specshow(spec, x_axis='time', y_axis='mel', sr=sr, fmax=8000, ax=ax)\n",
        "            fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
        "\n",
        "\n",
        "# class rav_prep:\n",
        "#     '''\n",
        "#     01 = neutral, \n",
        "#     02 = calm,  -\n",
        "#     03 = happy, \n",
        "#     04 = sad, \n",
        "#     05 = angry, \n",
        "#     06 = fearful, \n",
        "#     07 = disgust,  -\n",
        "#     08 = surprised -\n",
        "#     '''\n",
        "#     @staticmethod\n",
        "#     def correct_data_type(path):\n",
        "#         if (path.split('/')[-1].split('-')[0] == '03') and (path.split('/')[-1].split('-')[1] == '01') and (path.split('/')[-1].split('-')[2] in ['01', '03', '04', '05', '06']):\n",
        "#           return True\n",
        "#         else:\n",
        "#           return False\n",
        "    \n",
        "#     @staticmethod\n",
        "#     def filter(path):\n",
        "#       counter = 0\n",
        "#       for i in glob.glob(path):\n",
        "#         if rav_prep.correct_data_type(i):\n",
        "#           continue\n",
        "#         elif rav_prep.correct_data_type(i) != True:\n",
        "#           sh.move(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/A_removed_files')\n",
        "#           counter += 1\n",
        "#           continue\n",
        "#       print(f'removed {counter} files')\n",
        "\n",
        "    # @staticmethod\n",
        "    # def move_ravdess_colab(path):                                               # colab google drive paths\n",
        "    #   for i in glob.glob(path):\n",
        "    #     if i.split('/')[-1].split('-')[2] == '05':\n",
        "    #       sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/angry')\n",
        "        \n",
        "    #     elif i.split('/')[-1].split('-')[2] == '06':\n",
        "    #       sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/fear')\n",
        "\n",
        "    #     elif i.split('/')[-1].split('-')[2] == '03':\n",
        "    #       sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/happy')\n",
        "\n",
        "    #     elif i.split('/')[-1].split('-')[2] == '01':\n",
        "    #       sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/neutral')\n",
        "\n",
        "    #     elif i.split('/')[-1].split('-')[2] == '04':\n",
        "    #       sh.copy(i, '/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/sad')\n",
        "\n",
        "\n",
        "class ds_create:\n",
        "    \n",
        "    @staticmethod    \n",
        "    def label_from_bpath(bpath):                                                       # probably will not be used\n",
        "        return bpath.decode('utf-8').split('\\\\')[-2]\n",
        "\n",
        "    @staticmethod\n",
        "    def slices_for_onelabel(path, label):                                              \n",
        "        paths = glob.glob(path + label + '/*wav')\n",
        "\n",
        "        labels = [label_to_ind[label]]*len(paths)\n",
        "\n",
        "        return paths , labels\n",
        "\n",
        "    @staticmethod\n",
        "    def dfpremel(path):\n",
        "        data, sr = aud_util.loadaud(path, sr=16000, mono=True)\n",
        "        data, sr = aud_util.pad_trunc(data, sr, max_ms)                                \n",
        "        mel = aud_img.melspec(data, sr)\n",
        "        return mel\n",
        "    \n",
        "    @staticmethod\n",
        "    def dfpremfcc(path):\n",
        "        data, sr = aud_util.loadaud(path, sr=16000, mono=True)\n",
        "        data, sr = aud_util.pad_trunc(data, sr, max_ms)                                \n",
        "        mel = aud_img.mfcc(data, sr)\n",
        "        return mel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu-N6Za66BPG",
        "outputId": "afae6ca1-ab4e-4e90-ceec-5758792f9795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "removed 0 files\n"
          ]
        }
      ],
      "source": [
        "# '''\n",
        "# organising ravdess data \n",
        "# Done once only, by the time you see this cell, it probably was already run, so you can ignore it \n",
        "# as all the revdess files have already been organised into the sub-emotion folder in the google drive, in\n",
        "# /content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data\n",
        "\n",
        "# '''\n",
        "\n",
        "# rav_prep.filter('/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/Altogether/*.wav')\n",
        "# rav_prep.move_ravdess_colab('/content/drive/MyDrive/NLP/RAVDESS_altogether/modified_data/Altogether/*.wav')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM7Tux-YGMrp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "BUILDING DATASET PIPELINE (both original data and ravdess)\n",
        "\n",
        "'_o' means original data, excluding any extra data\n",
        "\n",
        " - colab, (btw doing this on a windows machine will break completely because of their stupid backward slash)\n",
        "'''\n",
        "\n",
        "angry_o, _0 =   ds_create.slices_for_onelabel(r'Data/NLP Training Dataset/', 'angry')\n",
        "fear_o, _1 =    ds_create.slices_for_onelabel(r'Data/NLP Training Dataset/', 'fear')\n",
        "happy_o, _2 =   ds_create.slices_for_onelabel(r'Data/NLP Training Dataset/', 'happy')\n",
        "neutral_o, _3 = ds_create.slices_for_onelabel(r'Data/NLP Training Dataset/', 'neutral')\n",
        "sad_o, _4 =     ds_create.slices_for_onelabel(r'Data/NLP Training Dataset/', 'sad')\n",
        "\n",
        "angry_r, r_0 =   ds_create.slices_for_onelabel(r'Data/RAVDESS Dataset Sorted/', 'angry')\n",
        "fear_r, r_1 =    ds_create.slices_for_onelabel(r'Data/RAVDESS Dataset Sorted/', 'fear')\n",
        "happy_r, r_2 =   ds_create.slices_for_onelabel(r'Data/RAVDESS Dataset Sorted/', 'happy')\n",
        "neutral_r, r_3 = ds_create.slices_for_onelabel(r'Data/RAVDESS Dataset Sorted/', 'neutral')\n",
        "sad_r, r_4 =     ds_create.slices_for_onelabel(r'Data/RAVDESS Dataset Sorted/', 'sad')\n",
        "\n",
        "\n",
        "slices = angry_o + fear_o + happy_o + neutral_o + sad_o + angry_r + fear_r + happy_r + neutral_r + sad_r\n",
        "labels = _0 + _1 + _2 + _3 + _4 + r_0 + r_1 + r_2 + r_3 + r_4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFG2j4pS3GxJ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Dataframe style\n",
        "\n",
        "using tf.stack later lol\n",
        "'''\n",
        "\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "df['relative_audio_paths'] = slices\n",
        "df['int_labels'] = labels\n",
        "df['1hot_labels'] = list(to_categorical(labels))\n",
        "\n",
        "df['imgs_3c'] = list(map(ds_create.dfpremel, slices))\n",
        "\n",
        "\n",
        "df = sk.utils.shuffle(df)\n",
        "df.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "icL9-cYI3GxJ",
        "outputId": "b23915d7-7d5c-442b-b3ef-cf2c5d7e6d41"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e45ad8a3-3a98-4362-97bd-5646eebbc0f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relative_audio_paths</th>\n",
              "      <th>int_labels</th>\n",
              "      <th>1hot_labels</th>\n",
              "      <th>imgs_3c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
              "      <td>4</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>[[[38.78178342333567, 38.78178342333567, 38.78...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
              "      <td>[[[42.12918, 42.12918, 42.12918], [42.12918, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
              "      <td>2</td>\n",
              "      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
              "      <td>[[[72.87353194359729, 72.87353194359729, 72.87...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
              "      <td>4</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>[[[57.963776, 57.963776, 57.963776], [57.84632...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/NLP/NLP Training Datase...</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
              "      <td>[[[49.65379750655458, 49.65379750655458, 49.65...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e45ad8a3-3a98-4362-97bd-5646eebbc0f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e45ad8a3-3a98-4362-97bd-5646eebbc0f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e45ad8a3-3a98-4362-97bd-5646eebbc0f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                relative_audio_paths  int_labels  \\\n",
              "0  /content/drive/MyDrive/NLP/NLP Training Datase...           4   \n",
              "1  /content/drive/MyDrive/NLP/NLP Training Datase...           3   \n",
              "2  /content/drive/MyDrive/NLP/NLP Training Datase...           2   \n",
              "3  /content/drive/MyDrive/NLP/NLP Training Datase...           4   \n",
              "4  /content/drive/MyDrive/NLP/NLP Training Datase...           0   \n",
              "\n",
              "                 1hot_labels  \\\n",
              "0  [0.0, 0.0, 0.0, 0.0, 1.0]   \n",
              "1  [0.0, 0.0, 0.0, 1.0, 0.0]   \n",
              "2  [0.0, 0.0, 1.0, 0.0, 0.0]   \n",
              "3  [0.0, 0.0, 0.0, 0.0, 1.0]   \n",
              "4  [1.0, 0.0, 0.0, 0.0, 0.0]   \n",
              "\n",
              "                                             imgs_3c  \n",
              "0  [[[38.78178342333567, 38.78178342333567, 38.78...  \n",
              "1  [[[42.12918, 42.12918, 42.12918], [42.12918, 4...  \n",
              "2  [[[72.87353194359729, 72.87353194359729, 72.87...  \n",
              "3  [[[57.963776, 57.963776, 57.963776], [57.84632...  \n",
              "4  [[[49.65379750655458, 49.65379750655458, 49.65...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3hBgoy43GxK",
        "outputId": "40f1580f-bec7-4ffb-80c6-507d1cc6f6b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128, 126, 3)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.iloc[0,3].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0_tkT0_8SMW"
      },
      "source": [
        "#original baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TlqaMX-3GxL",
        "outputId": "b4b2739d-71d5-4c41-ec94-4dc566005760"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128, 126, 3)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_shape = df.iloc[0,3].shape\n",
        "input_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0bzHjuMOf8T",
        "outputId": "4c17c699-5bbb-4a55-d2ab-6c60c3b71933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 126, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetv2-m (Functiona  (None, None, None, 1280)  53150388 \n",
            " l)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 20480)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               5243136   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 58,427,065\n",
            "Trainable params: 58,135,033\n",
            "Non-trainable params: 292,032\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "m model\n",
        "'''\n",
        "xin = Input(input_shape)\n",
        "\n",
        "prenet = tf.keras.applications.efficientnet_v2.EfficientNetV2M(weights='imagenet', include_top=False)#, input_shape=input_shape)\n",
        "x = prenet(xin)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='swish')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation='swish')(x)\n",
        "\n",
        "\n",
        "xout = Dense(5, activation='softmax')(x)\n",
        "\n",
        "ownm = Model(xin, xout)\n",
        "ownm.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), metrics=['acc', tfa.metrics.F1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
        "ownm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6-Qzo5J3GxL",
        "outputId": "dbf45ca1-9b37-466a-aa2d-2a827710c2b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 126, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetv2-s (Functiona  (None, None, None, 1280)  20331360 \n",
            " l)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 20480)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               5243136   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,608,037\n",
            "Trainable params: 25,454,165\n",
            "Non-trainable params: 153,872\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "s model\n",
        "'''\n",
        "xin = Input(input_shape)\n",
        "\n",
        "prenet = tf.keras.applications.efficientnet_v2.EfficientNetV2S(weights='imagenet', include_top=False)#, input_shape=input_shape)\n",
        "x = prenet(xin)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='swish')(x)\n",
        "x = Dense(128, activation='swish')(x)\n",
        "\n",
        "\n",
        "xout = Dense(5, activation='softmax')(x)\n",
        "\n",
        "own = Model(xin, xout)\n",
        "own.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), metrics=['acc', tfa.metrics.F1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
        "own.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ7XBRJw3GxM"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.1, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint(r'/content/drive/MyDrive/NLP/Zhihaos stuff/effinet_v2m', monitor='val_loss', verbose=0, save_best_only=True)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcgEkPN23GxM"
      },
      "outputs": [],
      "source": [
        "history = ownm.fit(\n",
        "    x=tf.stack(df['imgs_3c']),\n",
        "    y=tf.stack(df['1hot_labels']),\n",
        "    batch_size=batchs,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "ownm.save(r'/content/drive/MyDrive/NLP/Saved Models/modelM_1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5scV_W8n07yj",
        "outputId": "b592f697-0135-48d7-9dbe-3bc97e49204a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-s_notop.h5\n",
            "82427904/82420632 [==============================] - 3s 0us/step\n",
            "82436096/82420632 [==============================] - 3s 0us/step\n",
            "Epoch 1/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 1.3389 - acc: 0.5965 - f1_score: 0.5298INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 138s 2s/step - loss: 1.3389 - acc: 0.5965 - f1_score: 0.5298 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.9738 - acc: 0.8032 - f1_score: 0.7856INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 108s 2s/step - loss: 0.9738 - acc: 0.8032 - f1_score: 0.7856 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.8717 - acc: 0.8728 - f1_score: 0.8654INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 109s 2s/step - loss: 0.8717 - acc: 0.8728 - f1_score: 0.8654 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7913 - acc: 0.9280 - f1_score: 0.9202INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 110s 2s/step - loss: 0.7913 - acc: 0.9280 - f1_score: 0.9202 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7620 - acc: 0.9464 - f1_score: 0.9430INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 108s 2s/step - loss: 0.7620 - acc: 0.9464 - f1_score: 0.9430 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7555 - acc: 0.9494 - f1_score: 0.9455INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 109s 2s/step - loss: 0.7555 - acc: 0.9494 - f1_score: 0.9455 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7454 - acc: 0.9533 - f1_score: 0.9517INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 109s 2s/step - loss: 0.7454 - acc: 0.9533 - f1_score: 0.9517 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.7746 - acc: 0.9324 - f1_score: 0.9323 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7097 - acc: 0.9732 - f1_score: 0.9719INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 108s 2s/step - loss: 0.7097 - acc: 0.9732 - f1_score: 0.9719 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.7036 - acc: 0.9782 - f1_score: 0.9765INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 110s 2s/step - loss: 0.7036 - acc: 0.9782 - f1_score: 0.9765 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.7042 - acc: 0.9755 - f1_score: 0.9740 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.6903 - acc: 0.9867 - f1_score: 0.9860INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 109s 2s/step - loss: 0.6903 - acc: 0.9867 - f1_score: 0.9860 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.6916 - acc: 0.9840 - f1_score: 0.9834 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "69/69 [==============================] - 14s 198ms/step - loss: 0.6937 - acc: 0.9821 - f1_score: 0.9825 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.6809 - acc: 0.9892 - f1_score: 0.9886INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 111s 2s/step - loss: 0.6809 - acc: 0.9892 - f1_score: 0.9886 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.6839 - acc: 0.9879 - f1_score: 0.9879 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.6823 - acc: 0.9897 - f1_score: 0.9892 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.6749 - acc: 0.9934 - f1_score: 0.9929INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/Model Checkpoints/assets\n",
            "69/69 [==============================] - 109s 2s/step - loss: 0.6749 - acc: 0.9934 - f1_score: 0.9929 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "69/69 [==============================] - 14s 199ms/step - loss: 0.6882 - acc: 0.9856 - f1_score: 0.9855 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "69/69 [==============================] - 14s 198ms/step - loss: 0.6869 - acc: 0.9888 - f1_score: 0.9856 - lr: 0.0010\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/NLP/Saved Models/model2_5/assets\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Ensemble Learning modelm_5 left to be trained\n",
        "'''\n",
        "\n",
        "for count in range(5,6): # already ran 1-3 and saved models\n",
        "    \n",
        "    xin = Input(input_shape)\n",
        "\n",
        "    prenet = tf.keras.applications.efficientnet_v2.EfficientNetV2S(weights='imagenet', include_top=False)#, input_shape=input_shape)\n",
        "    x = prenet(xin)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(256, activation='swish')(x)\n",
        "    x = Dense(128, activation='swish')(x)\n",
        "\n",
        "\n",
        "    xout = Dense(5, activation='softmax')(x)\n",
        "\n",
        "    own = Model(xin, xout)\n",
        "    own.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), metrics=['acc', tfa.metrics.F1Score(num_classes=5, average='weighted', threshold=0.5)])\n",
        "    \n",
        "    callbacks_en = [\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=3, factor=0.1, verbose=1),\n",
        "    # tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint(r'/content/drive/MyDrive/NLP/Saved Models/Model Checkpoints', monitor='loss', verbose=0, save_best_only=True)\n",
        "    ]\n",
        "\n",
        "\n",
        "    history = own.fit(\n",
        "        x=tf.stack(df['imgs_3c']),\n",
        "        y=tf.stack(df['1hot_labels']),\n",
        "        batch_size=batchs,\n",
        "        epochs=epochs, \n",
        "        #validation_split=0.2,\n",
        "        callbacks=callbacks_en,\n",
        "    )   \n",
        "    own.save(r'/content/drive/MyDrive/NLP/Saved Models/model2_' + str(count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwv-_DMS1kbr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Ensemble Learning effinet v2s\n",
        "'''\n",
        "model_1 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model_1') # val_f1_score is 0.8869\n",
        "model_2 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model_2') # 0.8894\n",
        "model_3 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model_3') # 0.8956\n",
        "model_4 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model_4') # 0.8736\n",
        "model_5 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model_5') # 0.8845"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uZ6HyPmCc2w"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Ensemble Learning effinet v2s no val_split\n",
        "'''\n",
        "modelm_1 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model2_1')\n",
        "modelm_2 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model2_2')\n",
        "modelm_3 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model2_3') \n",
        "modelm_4 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model2_4')\n",
        "modelm_5 = load_model(r'/content/drive/MyDrive/NLP/Saved Models/model2_5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qadd6XzL8E8R"
      },
      "source": [
        "# Generating the test preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tFlMT1x3GxM"
      },
      "outputs": [],
      "source": [
        "tst = ds_create.dfpremel(r'/content/drive/MyDrive/NLP/NLP Training Dataset/ASR Training Dataset/fear/00530e07e3.wav')\n",
        "tst = ds_create.dup_channel(tst)\n",
        "tst = np.expand_dims(tst, axis=0)                                             # EXPAND DIMS OF FIRST DIMENSION ARGHHHHHH\n",
        "pred = own.predict(tst)\n",
        "pred = np.argmax(pred)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6goTowRgPN4",
        "outputId": "92d93c86-29f0-44e1-d24d-24df680bf05c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1eebd177d0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "own.load_weights(r'/content/drive/MyDrive/NLP/Zhihaos stuff/effinet_v2s_nodrop')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClBa4ZF3fEIN",
        "outputId": "f03c43b5-52a8-4759-bf1e-48c082c632fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "137/137 [==============================] - 20s 47ms/step - loss: 0.6959 - acc: 0.9794 - f1_score: 0.9794\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.6958815455436707, 0.9793767333030701, 0.9794116616249084]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "own.evaluate(\n",
        "    x=tf.stack(df['imgs_3c']),\n",
        "    y=tf.stack(df['1hot_labels'])\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDMxvMdP3GxN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Generating the qualifying csv file\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "class test_gen:\n",
        "    @staticmethod\n",
        "    def path_gen(path):                                              #for zhihao's local pc\n",
        "        paths = os.listdir(path)\n",
        "        paths = list(map(lambda x : 'DATA_NLP_TIL\\\\.qualifying_test\\\\'+x , paths))\n",
        "\n",
        "        return paths \n",
        "\n",
        "    @staticmethod\n",
        "    def path_gen_colab(path):                                              #for colab, zhihaos\n",
        "        paths = os.listdir(path)\n",
        "        paths = list(map(lambda x : '/content/drive/MyDrive/NLP/NLP Interim Dataset/NLP/'+x , paths))\n",
        "\n",
        "        return paths \n",
        "\n",
        "    @staticmethod\n",
        "    def path_to_mel(path):\n",
        "        ccc = ds_create.dfpremel(path)\n",
        "        return ccc\n",
        "    \n",
        "    @staticmethod\n",
        "    def path_to_mfcc(path):\n",
        "        ccc = ds_create.dfpremfcc(path)\n",
        "        return ccc\n",
        "\n",
        "    @staticmethod\n",
        "    def int_to_label(int):\n",
        "        return ind_to_label[int]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QJXAxyVMp7y"
      },
      "outputs": [],
      "source": [
        "q_df = pd.DataFrame()\n",
        "paths = sorted(glob.glob(r'/content/drive/MyDrive/NLP/NLP Interim Dataset/NLP/*.wav'))\n",
        "q_data = list(map(test_gen.path_to_mel, paths))\n",
        "\n",
        "q_data = tf.stack(q_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYh2V6ZkBA22"
      },
      "outputs": [],
      "source": [
        "# preds = own.predict(q_data)\n",
        "# preds = np.argmax(preds, axis=1)\n",
        "'''\n",
        "Ensemble Learning\n",
        "'''\n",
        "pred_1 = modelm_1.predict(q_data)\n",
        "pred_2 = modelm_2.predict(q_data)\n",
        "pred_3 = modelm_3.predict(q_data)\n",
        "pred_4 = modelm_4.predict(q_data)\n",
        "pred_5 = modelm_5.predict(q_data)\n",
        "\n",
        "pred_comb = pred_1 * 0.2 + pred_2 * 0.2 + pred_3 * 0.2 + pred_4 * 0.2 + pred_5 * 0.2\n",
        "pred_comb = np.argmax(pred_comb, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFypbX_xvmLn",
        "outputId": "a8f9877b-234e-49ab-8124-e805e7d6cfc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(600,)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMiSafBkOTfp"
      },
      "outputs": [],
      "source": [
        "q_df['paths'] = sorted(os.listdir(r'/content/drive/MyDrive/NLP/NLP Interim Dataset/NLP/'))\n",
        "q_df['labels'] = list(map(\n",
        "    test_gen.int_to_label,\n",
        "    list(pred_comb)\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "rbFehkxqUGss",
        "outputId": "c9172d9e-28a1-4a9b-f5ce-127ccfbc7dd1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b4c5f51a-f209-4aa2-89ae-58152e0b64cf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paths</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00ae09ba94.wav</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00f2a00f1f.wav</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>012822b908.wav</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0144091c26.wav</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0145cb0279.wav</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4c5f51a-f209-4aa2-89ae-58152e0b64cf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4c5f51a-f209-4aa2-89ae-58152e0b64cf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4c5f51a-f209-4aa2-89ae-58152e0b64cf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            paths labels\n",
              "0  00ae09ba94.wav  happy\n",
              "1  00f2a00f1f.wav  angry\n",
              "2  012822b908.wav   fear\n",
              "3  0144091c26.wav    sad\n",
              "4  0145cb0279.wav    sad"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcxfIZE2UlkK"
      },
      "outputs": [],
      "source": [
        "q_df.to_csv(r'/content/drive/MyDrive/NLP/Zhihao nlp preds/qualifiers_ensemble3.csv', header=False, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copy of zhihao_nlp_2022TIL_(ignore othernotebook).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "38c7ce74dd526bc9e84fd0682f6c1ac8fcd6c4cb0e87d36fcf4e0e214217cc07"
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
